#!/usr/bin/env bash

function wait_on_deployment() {
  local dep="$1"
  local namespace="$2"
  local retries=30 # 2 minute timeout

  while ! kubectl rollout status -w "deployment/${dep}" --namespace=${namespace}; do
      sleep 6
      retries=$((retries - 1))
      if [[ $retries == 0 ]]; then
        >&2 echo "FAIL: Failed to rollout deployloyment \"$dep\""
        exit 1
      fi
      echo "retrying check rollout status for deployment \"$dep\"..."
  done

  echo "Successfully rolled out deployment \"$dep\" in namespace \"$namespace\""
}

function check_subscription_passes() {
  local sub="$1"
  local namespace="$2"
  local packageName="$3"
  local retries=90 # 3 minute timeout

  echo "Checking subscription \"$sub\" reaches \"AtLatestKnown\" state"
  while ! kubectl get "subscription/${sub}" --namespace=${namespace} -o json | jq .status -e &> /dev/null; do
      sleep 6
      retries=$((retries - 1))
      if [[ $retries == 0 ]]; then
        >&2 echo "FAIL: Failed to pick up subscription \"$sub\""
        kubectl get "subscription/${sub}" --namespace=${namespace} -o yaml
        # Check if operator package is accessible on cluster, if not check the operator registry pod and exit
        if ! kubectl get packagemanifests --namespace=${namespace} | tail -n +2 | grep ${packageName} &> /dev/null; then
          echo "Package ${packageName} could not be found. Checking operator-registry logs:"
          kubectl logs $(get_first_matching_pod "$packageName-ocs" "$namespace") --namespace=${namespace}
          exit 1
        fi
        echo "catalog-operator logs:"
        if [[ "$DISTRO_TYPE" == "$DISTRO_TYPE_OPENSHIFT" ]]; then
          kubectl logs $(get_first_matching_pod catalog-operator- openshift-operator-lifecycle-manager) --namespace="openshift-operator-lifecycle-manager"
        else
          kubectl logs $(get_first_matching_pod catalog-operator- olm) --namespace="olm"
        fi
        echo "operator-registry logs:"
        kubectl logs $(get_first_matching_pod "$packageName-ocs" "$namespace") --namespace=${namespace}
        exit 1
      fi
      echo "retrying check of subscription status for \"$sub\"..."
  done

  # Check that subscription status is set to "AtLatestKnown"
  while ! [[ `kubectl get "subscription/${sub}" --namespace=${namespace} -o json | jq .status.state` == \"AtLatestKnown\" ]]; do
      sleep 6
      retries=$((retries - 1))
      if [[ $retries == 0 ]]; then
        >&2 echo "FAIL: Failed to reach "AtLatestKnown" subscription status for \"$sub\""
        kubectl get "subscription/${sub}" --namespace=${namespace} -o yaml
        installplan="`kubectl get subscription/${sub} --namespace=${namespace} -o json | jq .status.installplan.name`"
        >&2 echo "installplan status for \"$installplan\":"
        kubectl get "installplan/${installplan}" --namespace=${namespace} -o yaml
        exit 1
      fi
      echo "subscription status for \"$sub\": `kubectl get "subscription/${sub}" --namespace=${namespace} -o json | jq .status.state`"
  done

  echo "Successfully installed subscription \"$sub\" in namespace \"$namespace\""
}

function check_csv_passes() {
  local csv="$1"
  local namespace="$2"
  local installmode="$3"
  local retries=30 # 5 minute timeout
  
  echo "Check that clusterserviceversion \"$csv\" status is set to \"Succeeded\""
  while ! [[ `kubectl get "clusterserviceversion/${csv}" --namespace=${namespace} -o json | jq --raw-output .status.phase` == "Succeeded" ]]; do
      sleep 10
      retries=$((retries - 1))
      if [[ $retries == 0 ]]; then
        >&2 echo "failed to reach "Succeeded" clusterserviceversion status for \"$csv\""
        # Print out CSV's 'status' section on error
        kubectl get "clusterserviceversion/${csv}" --namespace=${namespace} -o jsonpath="{.status}"
        # Check operator pod(s) logs
        get_operator_logs "$csv" "$namespace"
        exit 1
      fi
      echo "clusterserviceversion phase for \"$csv\": `kubectl get "clusterserviceversion/${csv}" --namespace=${namespace} -o json | jq .status.phase`"
  done

  echo "Successfully installed clusterserviceversion \"$csv\" in namespace \"$namespace\""
}

function check_cluster_available() {
  local cluster_name="$1"

  if [[ -z "$KUBECONFIG" ]]; then
    echo "KUBECONFIG not set"
    return 1
  fi

  if ! kubectl cluster-info; then
    echo "Cluster \"${cluster_name}.devcluster.openshift.com\" unreachable."
    return 1
  fi
}

# apply_objects_incluster creates in-cluster objects from manifests in
# $1. Objects are created in a particular order so those a CSV needs are
# present at install time:
# Secrets -> OperatorGroups -> all other Kinds
function apply_objects_incluster() {
  local deploy_dir="$1"

  declare -a secrets op_groups others
  local kind
  for f in $(find "$deploy_dir" -maxdepth 1 -name "*.yaml"); do
    kind="$(yq r "$f" "kind")"
    if [[ "$kind" == "Secret" ]]; then
      secrets+=("$f")
    elif [[ "$kind" == "OperatorGroup" ]]; then
      op_groups+=("$f")
    else
      others+=("$f")
    fi
  done
  for f in ${secrets[@]} ${op_groups[@]} ${others[@]}; do
    kubectl apply -f "$f"
  done
}

# delete_objects_incluster deletes cluster resources using manifests in $1.
function delete_objects_incluster() {
  local deploy_dir="$1"
  local namespace="$2"

  for f in $(find "$deploy_dir" -maxdepth 1 -name "*.yaml" -print); do
    kubectl delete --ignore-not-found=true "$(yq r "$f" "kind" | awk '{ print tolower($0) }')" "$(yq r "$f" "metadata.name")" --namespace="$namespace" --wait=true
  done
}

function delete_namespace() {
    local namespace="$1"
    local cleanmode="$2"
    local OK="\033[0;32m"
    local WARN="\033[0;33m"
    local NC="\033[0m"

    if [[ "$cleanmode" == "FORCE" ]]; then
      printf "Deleting namespace %s\t[ ${WARN} Processing ${NC} ]\n" | expand  -t 50 >&2;
    fi
    # for sure
    kubectl delete pod,deployment,subs,pvc,operatorgroup,service,ing -all -n "$namespace"  > /dev/null 2>&1
    kubectl delete ns "$namespace"  > /dev/null 2>&1
    if [[ "$cleanmode" == "FORCE" ]]; then
      printf "Deleting namespace %s\t[ ${OK} OK ${NC} ]\n" | expand  -t 50 >&2
    fi
}

function uninstall_olm() {
    local OK="\033[0;32m"
    local WARN="\033[0;33m"
    local NC="\033[0m"


    printf "Uninstall olm %s\t[ ${WARN} Processing ${NC} ]\n" | expand  -t 50 >&2;
    operator-sdk olm uninstall > /dev/null
    printf "Uninstall olm %s\t[ ${OK} OK ${NC} ]\n" | expand  -t 50 >&2
}

function log_operator_state() {
  local log_file="$1"
  local var=$(declare -p "$2")
  eval "declare -A objects="${var#*=}
  local namespace="$3"

  local obj_list obj
  for k in "${!objects[@]}"; do
    obj_list="${objects[$k]}"
    for obj in $obj_list; do
      >&2 echo -e "\n${k}: $(echo "$obj" | cut -d' ' -f1)\n" >> "$log_file"
      kubectl describe "$k" $obj --namespace="$namespace" >> "$log_file" 2>&1 || true
      >&2 echo -e "\n---\n" >> "$log_file"
    done
  done
  kubectl describe pods --namespace="$namespace" >> "$log_file" 2>&1
}

function get_operator_logs() {
  local csv="$1"
  local namespace="$2"

  # Get a list of pods and containers via the CSV
  declare -a operator_pods
  declare -A pods_containers
  local pod_counter=0
  while read -r deployment; do
    operator_pods[${pod_counter}]="$(echo "$deployment" | jq -r ".name")"
    pods_containers["${operator_pods[${pod_counter}]}"]="$(echo "$deployment" | jq '.spec.template.spec.containers' | jq -r '.[].name')"
    ((++pod_counter))
  done < <(kubectl get csv "$csv" --namespace="$namespace" -o json | jq -r ".spec.install.spec.deployments" | jq -c '.[]')

  # Print out logs for all pods deployed by the CSV
  local cluster_pods="$(kubectl get pods --namespace="$namespace" -o=name)"
  for pod in "${operator_pods[@]}"; do
    if ! echo "$cluster_pods" | grep "$pod" &> /dev/null; then
      echo "Pod "$pod" listed in CSV was not found in namespace "$namespace""
    else
      for container in `echo ${pods_containers["$pod"]}`; do
        # Skip scorecard-proxy container since it's not part of the operator
        if [[ "$container" == "scorecard-proxy" ]]; then
          continue
        fi
        echo "Logs for container "$container" in pod "$pod":"
        kubectl logs $(get_first_matching_pod "$pod-" "$namespace") -c "$container" --namespace="$namespace"
        echo ""
      done
    fi
  done
}

function get_first_matching_pod() {
  local prefix="$1"
  local namespace="$2"

  echo "$(kubectl get pods --namespace="$namespace" -o name | awk -v prefix="$prefix" '$1 ~ prefix' | head -n 1)"
}

function get_op_namespace() {
  local csv_file="$1"
  local op_name="$2"

  declare -A modes
  while read -r mode; do
    modes["$(echo "$mode" | jq -r ".type")"]="$(echo "$mode" | jq -r ".supported")"
  done < <(yq r --tojson "$csv_file" "spec.installModes" | jq -c '.[]')

  if [[ ${modes["AllNamespaces"]} == "true" ]]; then
    echo "$op_name"
  elif [[ ${modes["SingleNamespace"]} == "true" ]]; then
    echo "$op_name"
  elif [[ ${modes["OwnNamespace"]} == "true" ]]; then
    echo "$op_name"
  else
    # Operators should have at least one of SingleNamespace, AllNamespaces, OwnNamespaces true.
    echo "Operators should have at least one of OwnNamespace, SingleNamespace or  AllNamespaces set to true" 1>&2
    exit 1
  fi
}

function get_channel_name() {
  local pkg_file="$1"
  local csv_name="$2"

  declare -A channels
  while read -r channel; do
    channels["$(echo "$channel" | jq -r ".currentCSV")"]="$(echo "$channel" | jq -r ".name")"
  done < <(yq r --tojson "$pkg_file" "channels" | jq -c '.[]')

  if [[ -n "${channels["$csv_name"]}" ]]; then
    echo ${channels["$csv_name"]}
  else
    echo "No matching channel exists for this CSV in the package file" 1>&2
    exit 1
  fi
}

function inject_kubeconfig_volume() {
  local csv_json="$1"
  local insert_path=".spec.install.spec.deployments[].spec.template.spec"
  local volume='{
    "name": "scorecard-kubeconfig",
    "secret": {
      "secretName": "scorecard-kubeconfig",
      "items": [
        {
          "key": "kubeconfig",
          "path": "config"
        }
      ]
    }}'

   echo "$csv_json" | jq "${insert_path} |= . + {volumes: (.volumes + [${volume}] ) }"
}

function inject_kubeconfig_secret_mount() {
  local csv_json=$1
  local insert_path=".spec.install.spec.deployments[].spec.template.spec.containers[]"
  local volume_mount='{name: "scorecard-kubeconfig", mountPath: "/scorecard-secret"}'

  jq "${insert_path} |= . + {volumeMounts: (.volumeMounts + [${volume_mount}] ) }"  <<< ${csv_json}
}

function inject_kubeconfig_env_var() {
  local csv_json=$1
  local insert_path=".spec.install.spec.deployments[].spec.template.spec.containers[]"
  local env_var='{name: "KUBECONFIG", value: "/scorecard-secret/config"}'

  jq "${insert_path} |= . + {env: (.env + [${env_var}] ) }"  <<< ${csv_json}
}

function inject_proxy_container {
  local csv_json=$1
  local proxy_image="$2"
  local insert_path=".spec.install.spec.deployments[].spec.template.spec"
  local proxy_container='{
    "name": "scorecard-proxy",
    "command": [
      "scorecard-proxy"
    ],
    "env": [
      {
        "name": "WATCH_NAMESPACE",
        "valueFrom": {
          "fieldRef": {
            "apiVersion": "v1",
            "fieldPath": "metadata.namespace"
          }
        }
      }
    ],
    "image": "'"$proxy_image"'",
    "imagePullPolicy": "Always",
    "ports": [
      {
        "name": "proxy",
        "containerPort": 8889
      }
    ]
  }'
  
  jq "${insert_path} |= . + {containers: (.containers + [${proxy_container}] ) }"  <<< ${csv_json}
}

function generate_kubeconfig_secret_file() {
  local namespace="$1"
  # Expected proxy url. The proxy is configured to listen on port 8889.
  local server="localhost:8889"
  # Username for scorecard namespaced owner reference.
  local username="$(echo '{"apiVersion":"","kind":"","name":"scorecard","uid":"","Namespace":"'$namespace'"}' | base64 | tr -d \\n)"
  # Kubeconfig used to communicate with the proxy.
  local kubeconfig="apiVersion: v1
kind: Config
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: http://${username}@${server}
  name: proxy-server
contexts:
- context:
    cluster: proxy-server
    user: admin/proxy-server
  name: $namespace/proxy-server
current-context: $namespace/proxy-server
preferences: {}
users:
- name: admin/proxy-server
  user:
    username: $username
    password: unused"
  local kubeconfig_base64="$(echo "$kubeconfig" | base64 | tr -d \\n)"

  cat <<EOF
apiVersion: v1
kind: Secret
metadata:
  name: scorecard-kubeconfig
  namespace: $namespace
  labels:
    operator: test
data:
  kubeconfig: $kubeconfig_base64
EOF
}

# Prepares an Operator (CSV) for testing with scorecard
function instrument_operator() {
  local csv="$1"
  local namespace="$2"
  local scorecard_proxy_image=${3:-"quay.io/operator-framework/scorecard-proxy:master"}

  local secret_yaml="$(generate_kubeconfig_secret_file $namespace)"

  local temp_secret=$(mktemp)
  echo "$secret_yaml" > $temp_secret
  kubectl apply -f $temp_secret

  check_csv_passes $csv $namespace

  local csv_json="$(kubectl get csv $csv -n $namespace -o json)"
  local csv_json="$(inject_kubeconfig_volume "$csv_json")"
  local csv_json="$(inject_kubeconfig_secret_mount "$csv_json")"
  local csv_json="$(inject_kubeconfig_env_var "$csv_json")"
  local csv_json="$(inject_proxy_container "$csv_json" "$scorecard_proxy_image")"

  local temp_csv=$(mktemp)
  echo "$csv_json" > $temp_csv

  kubectl apply --validate=false -f $temp_csv
}

# checks whether the CSV and it's sibling Deployments hav the scorecard-proxy side car
function check_scorecard_presence() {
  local csv="$1"
  local namespace="$2"
  retries=30 # 5 minute timeout
  
  echo "Check that clusterserviceversion \"$csv\" succeeded applied scorecard-proxy"
  while [ $retries -gt 0 ]; do
      sleep 10 

      csv_status=$(kubectl get csv ${csv} --namespace=${namespace} -o json | jq --raw-output .status.phase) 
      csv_scorecard_container=$(kubectl get csv ${csv} --namespace=${namespace} -o jsonpath="{$.spec.install.spec.deployments[*].spec.template.spec.containers[?(@.name=='scorecard-proxy')].command}")
      csv_has_scorecard=$([ -z "$csv_scorecard_container" ] && echo "False" || echo "True" )

      csv_deployments=$(kubectl get deployment -n $namespace -o json |  jq --raw-output ".items[] | select(.metadata.ownerReferences[].name==\"${csv}\") | .metadata.name")
      csv_deployments_have_scorecard="True"

      for deployment in $csv_deployments ; do
          deployment_has_scorecard=$(kubectl get deployment ${deployment} --namespace=${namespace} -o jsonpath="{$.spec.template.spec.containers[?(@.name=='scorecard-proxy')].command}")

          if [ -z "$deployment_has_scorecard" ] ; then 
            csv_deployments_have_scorecard="False"
          fi
      done

      echo "clusterserviceversion \"$csv\" is in phase: $csv_status"
      echo "clusterserviceversion \"$csv\" has scorecard container: $csv_has_scorecard"
      echo "clusterserviceversion \"$csv\" deployments have scorecard container: $csv_deployments_have_scorecard"

      if [[ "${csv_status}" == "Succeeded" && "$csv_has_scorecard" == "True" && "$csv_deployments_have_scorecard" == "True" ]] ; then
        echo "Clusterserviceversion \"$csv\" successfully applied scorecard-proxy in namespace \"$namespace\""
        return 0
      else
        retries=$((retries - 1))
      fi
  done

  >&2 echo "failed to reach "Succeeded" phase or find scorecard in clusterserviceversion \"$csv\""
  # Print out CSV's 'status' section on error
  kubectl get "clusterserviceversion/${csv}" --namespace=${namespace} -o jsonpath="{.status}"
  kubectl get csv ${csv} --namespace=${namespace} -o jsonpath="{$.spec.install.spec.deployments[*].spec.template.spec.containers[?(@.name=='scorecard-proxy')]}"
  # Check operator pod(s) logs
  get_operator_logs "$csv" "$namespace"
  return 1
}
