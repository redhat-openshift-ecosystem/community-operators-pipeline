{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About this repository The goal of the community operator pipeline framework project is to deploy and configure pipelines for various flavors of Kubernetes or Openshift clusters and publish multiple versions of operator indexes. Documentation contributions For changes in the documentation, please raise a PR against redhat-openshift-ecosystem/community-operators-pipeline:documentation-admin . Reporting Bugs Use the issue tracker in this repository to report bugs.","title":"Overview"},{"location":"#about-this-repository","text":"The goal of the community operator pipeline framework project is to deploy and configure pipelines for various flavors of Kubernetes or Openshift clusters and publish multiple versions of operator indexes.","title":"About this repository"},{"location":"#documentation-contributions","text":"For changes in the documentation, please raise a PR against redhat-openshift-ecosystem/community-operators-pipeline:documentation-admin .","title":"Documentation contributions"},{"location":"#reporting-bugs","text":"Use the issue tracker in this repository to report bugs.","title":"Reporting Bugs"},{"location":"framework/development/","text":"Development Overview Three main components are used in each pipeline GitHub action workflows Driver scripts opp-env.sh opp.sh helper scripts Ansible playbooks from upstream-community branch upstream directory Process GitHub action workflows are stored as ansible templates and during the Upgrade process Workflows are resolved using config files and applied to the running project. The testing and release pipelines are triggered by these workflows and followed by running Ansible playbook with various parameters. These parameters are controlled by opp.sh script and the most relevant part is function ExecParameters() . These parameters are passed to Ansible Playbook local.yaml The playbook is executed inside the container produced by quay.io/operator_testing/operator-test-playbooks:latest image. Production vs. Development The Codebase and development table shows various branches used for production and development . Let's summarize it in the table bellow Type GitHub action workflows and scripts Ansible playbooks Ansible playbook image Production ci/latest upstream-community quay.io/operator_testing/operator-test-playbooks:latest Development or staging ci/dev upstream-community-dev quay.io/operator_testing/operator-test-playbooks:dev GitHub action workflows and scripts The ci/latest and ci/dev branches can be used in both production and development projects by specifying the branch name in Upgrade workflow Ansible playbook images The latest and dev tag values are used in the project as stated in the configuration file of the project shown in Production operator repositories in pipline.image key. The latest tag is produced automatically by pushing changes into https://github.com/redhat-openshift-ecosystem/operator-test-playbooks in upstream-community branch. The dev tag is produced by manually triggering Github Action Build playbook image and choosing a branch that starts with upstream-community-* . More info in the script here . The developer should choose a name starting with upstream-community-* for the branch before doing development. Operator tools versions and upgrade The tools like operator-sdk , opm and others that are used in the pipeline process are installed via Ansible Playbooks. These tools are automatically installed in quay.io/operator_testing/operator-test-playbooks image or installed on-fly via ansible-playbook command. The tool versions are configured at the main local.yaml . The relevant part is shown below and it is self-explanatory: kind_version: v0.12.0 kind_kube_version: v1.21.1 operator_sdk_version: v1.25.2 operator_courier_version: 2.1.11 olm_version: 0.20.0 opm_version: v1.26.2 k8s_community_bundle_validator_version: v0.1.0 oc_version: 4.3.5 go_version: 1.13.7 jq_version: 1.6 yq_version: 2.2.1 umoci_version: v0.4.5 iib_version: v6.3.0 Ansible playbook example Ansible playbook parameters are controlled by opp.sh script via ExecParameters() . The following example shows how operator_info is created by Operator release pipeline ansible-playbook -i localhost, -e ansible_connection=local \\ upstream/local.yml \\ --tags reset_tools,operator_info \\ -e run_upstream=true \\ -e run_prepare_catalog_repo_upstream=true \\ -e catalog_repo=https://github.com/redhat-openshift-ecosystem/community-operators-pipeline -e catalog_repo_branch=main \\ -e operator_base_dir=/tmp/community-operators-for-catalog/operators \\ -e operators=auqa,cockroachdb \\ -e cluster_type=ocp \\ -e strict_cluster_version_labels=true \\ -e production_registry_namespace=quay.io/community-operators-pipeline Name Description ansible-playbook -i localhost, -e ansible_connection=local ansible command used in local run upstream/local.yml path to main playbook local.yaml in upstream dicrectory --tags reset_tools,operator_info reset_tools tag to install tools and operator_info to produce file contains info about operators -e run_upstream=true flag that upstream version is used -e run_prepare_catalog_repo_upstream=true flag that project and branch (see row bellow ) will be clonned to /tmp/community-operators-for-catalog -e catalog_repo=... -e catalog_repo_branch=... repo and branch that operators are stored -e operator_base_dir=/tmp/community-operators-for-catalog/operators path where operators are taken -e operators=auqa,cockroachdb list of operators to be processed -e cluster_type=ocp cluster type to be used ( k8s or ocp ) -e strict_cluster_version_labels=true flat that cluster version is checked -e production_registry_namespace=quay.io/community-operators-pipeline registry/namepace used for bundles already published to be combined for index Switch K8S and OCP One can switch between Kubernetes (k8s) and Openshift (ocp) setup by setting the value in Upgrade workflow. This is heavily used in development or staging projects by testing both scenarios before putting it in production.","title":"Development"},{"location":"framework/development/#development","text":"","title":"Development"},{"location":"framework/development/#overview","text":"Three main components are used in each pipeline GitHub action workflows Driver scripts opp-env.sh opp.sh helper scripts Ansible playbooks from upstream-community branch upstream directory","title":"Overview"},{"location":"framework/development/#process","text":"GitHub action workflows are stored as ansible templates and during the Upgrade process Workflows are resolved using config files and applied to the running project. The testing and release pipelines are triggered by these workflows and followed by running Ansible playbook with various parameters. These parameters are controlled by opp.sh script and the most relevant part is function ExecParameters() . These parameters are passed to Ansible Playbook local.yaml The playbook is executed inside the container produced by quay.io/operator_testing/operator-test-playbooks:latest image.","title":"Process"},{"location":"framework/development/#production-vs-development","text":"The Codebase and development table shows various branches used for production and development . Let's summarize it in the table bellow Type GitHub action workflows and scripts Ansible playbooks Ansible playbook image Production ci/latest upstream-community quay.io/operator_testing/operator-test-playbooks:latest Development or staging ci/dev upstream-community-dev quay.io/operator_testing/operator-test-playbooks:dev","title":"Production vs. Development"},{"location":"framework/development/#github-action-workflows-and-scripts","text":"The ci/latest and ci/dev branches can be used in both production and development projects by specifying the branch name in Upgrade workflow","title":"GitHub action workflows and scripts"},{"location":"framework/development/#ansible-playbook-images","text":"The latest and dev tag values are used in the project as stated in the configuration file of the project shown in Production operator repositories in pipline.image key. The latest tag is produced automatically by pushing changes into https://github.com/redhat-openshift-ecosystem/operator-test-playbooks in upstream-community branch. The dev tag is produced by manually triggering Github Action Build playbook image and choosing a branch that starts with upstream-community-* . More info in the script here . The developer should choose a name starting with upstream-community-* for the branch before doing development.","title":"Ansible playbook images"},{"location":"framework/development/#operator-tools-versions-and-upgrade","text":"The tools like operator-sdk , opm and others that are used in the pipeline process are installed via Ansible Playbooks. These tools are automatically installed in quay.io/operator_testing/operator-test-playbooks image or installed on-fly via ansible-playbook command. The tool versions are configured at the main local.yaml . The relevant part is shown below and it is self-explanatory: kind_version: v0.12.0 kind_kube_version: v1.21.1 operator_sdk_version: v1.25.2 operator_courier_version: 2.1.11 olm_version: 0.20.0 opm_version: v1.26.2 k8s_community_bundle_validator_version: v0.1.0 oc_version: 4.3.5 go_version: 1.13.7 jq_version: 1.6 yq_version: 2.2.1 umoci_version: v0.4.5 iib_version: v6.3.0","title":"Operator tools versions and upgrade"},{"location":"framework/development/#ansible-playbook-example","text":"Ansible playbook parameters are controlled by opp.sh script via ExecParameters() . The following example shows how operator_info is created by Operator release pipeline ansible-playbook -i localhost, -e ansible_connection=local \\ upstream/local.yml \\ --tags reset_tools,operator_info \\ -e run_upstream=true \\ -e run_prepare_catalog_repo_upstream=true \\ -e catalog_repo=https://github.com/redhat-openshift-ecosystem/community-operators-pipeline -e catalog_repo_branch=main \\ -e operator_base_dir=/tmp/community-operators-for-catalog/operators \\ -e operators=auqa,cockroachdb \\ -e cluster_type=ocp \\ -e strict_cluster_version_labels=true \\ -e production_registry_namespace=quay.io/community-operators-pipeline Name Description ansible-playbook -i localhost, -e ansible_connection=local ansible command used in local run upstream/local.yml path to main playbook local.yaml in upstream dicrectory --tags reset_tools,operator_info reset_tools tag to install tools and operator_info to produce file contains info about operators -e run_upstream=true flag that upstream version is used -e run_prepare_catalog_repo_upstream=true flag that project and branch (see row bellow ) will be clonned to /tmp/community-operators-for-catalog -e catalog_repo=... -e catalog_repo_branch=... repo and branch that operators are stored -e operator_base_dir=/tmp/community-operators-for-catalog/operators path where operators are taken -e operators=auqa,cockroachdb list of operators to be processed -e cluster_type=ocp cluster type to be used ( k8s or ocp ) -e strict_cluster_version_labels=true flat that cluster version is checked -e production_registry_namespace=quay.io/community-operators-pipeline registry/namepace used for bundles already published to be combined for index","title":"Ansible playbook example"},{"location":"framework/development/#switch-k8s-and-ocp","text":"One can switch between Kubernetes (k8s) and Openshift (ocp) setup by setting the value in Upgrade workflow. This is heavily used in development or staging projects by testing both scenarios before putting it in production.","title":"Switch K8S and OCP"},{"location":"framework/how-to-document/","text":"How to document Overview Documentation is published as mkdocs which is an extension to the markdown format. Documentation location Public-facing documentation is located at https://redhat-openshift-ecosystem.github.io/community-operators-prod/ https://k8s-operatorhub.github.io/community-operators/ https://redhat-openshift-ecosystem.github.io/community-operators-pipeline/ Where to edit a documentation User and maintainer documentation is at https://github.com/redhat-openshift-ecosystem/community-operators-pipeline/tree/documentation Admin documentation is located at https://github.com/redhat-openshift-ecosystem/community-operators-pipeline/tree/documentation-admin How to publish a documentation No need to publish documentation manually. It is automatically rendered overnight. If there is some urgent case and you need to publish documentation, please restart a Documentation workflow OCP documentation workflow https://github.com/redhat-openshift-ecosystem/community-operators-prod/actions/workflows/documentation.yaml K8S documentation workflow is at https://github.com/k8s-operatorhub/community-operators/actions/workflows/documentation.yaml Or for admin documentation use Documentation admin workflow at https://github.com/redhat-openshift-ecosystem/community-operators-pipeline/actions/workflows/documentation.yaml By restarting the last workflow you immediately release an actual documentation version Mkdocs image The image for mkdocs is located at quay.io/operator_testing/community-operators-mkdocs:latest . No need to build an image manually, but for a complete picture it is good to know that a corresponding Dockerfile is located at https://github.com/redhat-openshift-ecosystem/community-operators-pipeline/blob/documentation/Dockerfile.mkdoc To build the image the following command can be executed: podman build -f Dockerfile.mkdoc -t quay.io/operator_testing/community-operators-mkdocs:latest .","title":"How to document"},{"location":"framework/how-to-document/#how-to-document","text":"","title":"How to document"},{"location":"framework/how-to-document/#overview","text":"Documentation is published as mkdocs which is an extension to the markdown format.","title":"Overview"},{"location":"framework/how-to-document/#documentation-location","text":"Public-facing documentation is located at https://redhat-openshift-ecosystem.github.io/community-operators-prod/ https://k8s-operatorhub.github.io/community-operators/ https://redhat-openshift-ecosystem.github.io/community-operators-pipeline/","title":"Documentation location"},{"location":"framework/how-to-document/#where-to-edit-a-documentation","text":"User and maintainer documentation is at https://github.com/redhat-openshift-ecosystem/community-operators-pipeline/tree/documentation Admin documentation is located at https://github.com/redhat-openshift-ecosystem/community-operators-pipeline/tree/documentation-admin","title":"Where to edit a documentation"},{"location":"framework/how-to-document/#how-to-publish-a-documentation","text":"No need to publish documentation manually. It is automatically rendered overnight. If there is some urgent case and you need to publish documentation, please restart a Documentation workflow OCP documentation workflow https://github.com/redhat-openshift-ecosystem/community-operators-prod/actions/workflows/documentation.yaml K8S documentation workflow is at https://github.com/k8s-operatorhub/community-operators/actions/workflows/documentation.yaml Or for admin documentation use Documentation admin workflow at https://github.com/redhat-openshift-ecosystem/community-operators-pipeline/actions/workflows/documentation.yaml By restarting the last workflow you immediately release an actual documentation version","title":"How to publish a documentation"},{"location":"framework/how-to-document/#mkdocs-image","text":"The image for mkdocs is located at quay.io/operator_testing/community-operators-mkdocs:latest . No need to build an image manually, but for a complete picture it is good to know that a corresponding Dockerfile is located at https://github.com/redhat-openshift-ecosystem/community-operators-pipeline/blob/documentation/Dockerfile.mkdoc To build the image the following command can be executed: podman build -f Dockerfile.mkdoc -t quay.io/operator_testing/community-operators-mkdocs:latest .","title":"Mkdocs image"},{"location":"framework/overview/","text":"Framework Overview Introduction The goal of the community operator pipeline framework project is to deploy and configure pipelines for various flavors of Kubernetes or Openshift clusters and publish multiple versions of operator indexes. GitHub projects via GitHub Action technology are supported. Currently, the following projects are supported: Codebase and development Name Project Barnches Framework https://github.com/redhat-openshift-ecosystem/community-operators-pipeline ci/latest and ci/dev Ansible playbooks https://github.com/redhat-openshift-ecosystem/operator-test-playbooks upstream-community and upstream-community-dev User and maintainer documentation ( k8s and ocp ) https://github.com/redhat-openshift-ecosystem/community-operators-pipeline documentation Admin Documentation https://github.com/redhat-openshift-ecosystem/community-operators-pipeline documentation-admin Producion operator repositories Name Project Configuration Branch Kubernetes operators ( OperatorHub.io ) https://github.com/k8s-operatorhub/community-operators pipeline-config.yaml main OpenShift ( OCP ) operators https://github.com/redhat-openshift-ecosystem/community-operators-prod pipeline-config.yaml main Staging environment https://github.com/redhat-openshift-ecosystem/community-operators-pipeline pipeline-config-k8s.yaml and pipeline-config-ocp.yaml main Project structure Each project should contain the following directory structure $ tree -L 1 --sort=mtime -a . \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 .github \u251c\u2500\u2500 categories.json \u251c\u2500\u2500 upstream.Dockerfile \u251c\u2500\u2500 scripts \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 docs \u251c\u2500\u2500 README.md \u251c\u2500\u2500 config.yaml \u251c\u2500\u2500 ci \u251c\u2500\u2500 operators \u2514\u2500\u2500 .git The following table is describing each file or directory File Description README.md Main README file ci/ Project CI configration directory operators/ Directory contains all operators with the versions categories.json Config file with list of categories config.yaml Operator pipelines config file docs/ Contains file with pull request predefined template (generated by the framework. Don't change!!!) .github/workflows Directory contains GitHub Action workflows (generated by the framework. Don't change!!!) upstream.Dockerfile Dockerfile needed for CI pipeline (generated by the framework. Don't change!!!) scripts/ Temporary scripts directory (generated by the framework. Don't change!!!) Testing Pipeline via GitHub Action The testing pipeline flow chart is shown below flowchart TD A0[PR] A0 --> |push| B0(Operator test) A0 --> |push| C0(Operator CI) A0 --> |labeled| D0(Operator CI Labels) A0 --> |push| E0(DCO test) A0 --> |comment| F0[&#47retest] A0 --> |comment| G0[&#47merge possible] B0 --> |job|B10[lemon] B0 --> |job|B11[orange] B0 --> |job|B12[kiwi] E0 --> E1(DCO Workflow Complete) F0 --> F1(Issue Comments - retest) G0 --> G1(Issue Comments - merge) B10 --> B2(Operator Workflow Complete) B11 --> B2 B12 --> B2 B2 --> |handle label| B3[package-validated] B3 --> |handle label| B4[installation-validated] B4 --> |make comment| B5[&#47merge possible] C0 --> |job|C1[operator-automerge-enabled] C1 --> |handled label|C2[automerge-disabled] C2 --> |job|C20[operator-ci] C20 --> |handle label|C21[new-operator] C20 --> |handle label|C22[allow/operator-version-overwrite] C20 --> |handle label|C23[allow/operator-recreate] C20 --> |on error|C24[Fail + PR comment] D0 --> |handle label|D1[authorized-changes] D1 --> |make comment|G0 G1 --> G2[automerge] F1 --> |make comment|F20[&#47hold] F1 --> |make comment|F21[&#47hold cancel] E1 --> |handle label|E2[dco-failed] Download or edit Release Pipeline via GitHub Action The release pipeline flow charts are shown below Openshift(ocp) flowchart LR A0(PR-traffic-light) A0 --> A1(Remove Operator) A1 --> A2(Index check) A2 --> A3(Bundles) A3 --> A4(Index) A4 --> A5(Index verify) A5 --> A6(Slack notification) Download or edit Kubernetes(k8s) with publishing index into operatorhub.io flowchart LR A0(PR-traffic-light) A0 --> A1(Remove Operator) A1 --> A2(Index check) A2 --> A3(Bundles) A3 --> A4(Index) A4 --> A50(OHIO image) A4 --> A51(Index verify) A50 --> A6(Release / operatorhub.io) A6 --> A7(Slack notification) A51 --> A7 Download or edit","title":"Overview"},{"location":"framework/overview/#framework-overview","text":"","title":"Framework Overview"},{"location":"framework/overview/#introduction","text":"The goal of the community operator pipeline framework project is to deploy and configure pipelines for various flavors of Kubernetes or Openshift clusters and publish multiple versions of operator indexes. GitHub projects via GitHub Action technology are supported. Currently, the following projects are supported:","title":"Introduction"},{"location":"framework/overview/#codebase-and-development","text":"Name Project Barnches Framework https://github.com/redhat-openshift-ecosystem/community-operators-pipeline ci/latest and ci/dev Ansible playbooks https://github.com/redhat-openshift-ecosystem/operator-test-playbooks upstream-community and upstream-community-dev User and maintainer documentation ( k8s and ocp ) https://github.com/redhat-openshift-ecosystem/community-operators-pipeline documentation Admin Documentation https://github.com/redhat-openshift-ecosystem/community-operators-pipeline documentation-admin","title":"Codebase and development"},{"location":"framework/overview/#producion-operator-repositories","text":"Name Project Configuration Branch Kubernetes operators ( OperatorHub.io ) https://github.com/k8s-operatorhub/community-operators pipeline-config.yaml main OpenShift ( OCP ) operators https://github.com/redhat-openshift-ecosystem/community-operators-prod pipeline-config.yaml main Staging environment https://github.com/redhat-openshift-ecosystem/community-operators-pipeline pipeline-config-k8s.yaml and pipeline-config-ocp.yaml main","title":"Producion operator repositories"},{"location":"framework/overview/#project-structure","text":"Each project should contain the following directory structure $ tree -L 1 --sort=mtime -a . \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 .github \u251c\u2500\u2500 categories.json \u251c\u2500\u2500 upstream.Dockerfile \u251c\u2500\u2500 scripts \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 docs \u251c\u2500\u2500 README.md \u251c\u2500\u2500 config.yaml \u251c\u2500\u2500 ci \u251c\u2500\u2500 operators \u2514\u2500\u2500 .git The following table is describing each file or directory File Description README.md Main README file ci/ Project CI configration directory operators/ Directory contains all operators with the versions categories.json Config file with list of categories config.yaml Operator pipelines config file docs/ Contains file with pull request predefined template (generated by the framework. Don't change!!!) .github/workflows Directory contains GitHub Action workflows (generated by the framework. Don't change!!!) upstream.Dockerfile Dockerfile needed for CI pipeline (generated by the framework. Don't change!!!) scripts/ Temporary scripts directory (generated by the framework. Don't change!!!)","title":"Project structure"},{"location":"framework/overview/#testing-pipeline-via-github-action","text":"The testing pipeline flow chart is shown below flowchart TD A0[PR] A0 --> |push| B0(Operator test) A0 --> |push| C0(Operator CI) A0 --> |labeled| D0(Operator CI Labels) A0 --> |push| E0(DCO test) A0 --> |comment| F0[&#47retest] A0 --> |comment| G0[&#47merge possible] B0 --> |job|B10[lemon] B0 --> |job|B11[orange] B0 --> |job|B12[kiwi] E0 --> E1(DCO Workflow Complete) F0 --> F1(Issue Comments - retest) G0 --> G1(Issue Comments - merge) B10 --> B2(Operator Workflow Complete) B11 --> B2 B12 --> B2 B2 --> |handle label| B3[package-validated] B3 --> |handle label| B4[installation-validated] B4 --> |make comment| B5[&#47merge possible] C0 --> |job|C1[operator-automerge-enabled] C1 --> |handled label|C2[automerge-disabled] C2 --> |job|C20[operator-ci] C20 --> |handle label|C21[new-operator] C20 --> |handle label|C22[allow/operator-version-overwrite] C20 --> |handle label|C23[allow/operator-recreate] C20 --> |on error|C24[Fail + PR comment] D0 --> |handle label|D1[authorized-changes] D1 --> |make comment|G0 G1 --> G2[automerge] F1 --> |make comment|F20[&#47hold] F1 --> |make comment|F21[&#47hold cancel] E1 --> |handle label|E2[dco-failed] Download or edit","title":"Testing Pipeline via GitHub Action"},{"location":"framework/overview/#release-pipeline-via-github-action","text":"The release pipeline flow charts are shown below","title":"Release Pipeline via GitHub Action"},{"location":"project/config/","text":"Configuration The configuration file has to be stored in $PROJECT_DIR/ci/pipeline-config.yaml . There are three main sections production pipeline operatorhubio (for k8s ) Supported config options are explained for all sections below Production production: type: ocp bundle: registry: quay.io organization: openshift-community-operators index: registry: quay.io organization: openshift-community-operators name: catalog_tmp tags: - v4.8-db - v4.9-db - v4.10-db - v4.11 - v4.12 signature: enabled: 1 alias: registry.redhat.io/redhat/community-operator-index endpoint: https://community-signing-pipeline-prod.apps.pipelines-prod.ijdb.p1.openshiftapps.com mirror: enabled: 1 multiarch: base: registry.redhat.io/openshift4/ose-operator-registry base_tags: - v4.5 - v4.6 - v4.7 - v4.8 - v4.9 - v4.10 - v4.11 postfix: s registry: quay.io organization: redhat name: redhat----community-operator-index username: in: mavala out: redhat+iib_community test: installation_skip: - ack- - community-windows-machine-config-operator Name Description Possible options production.type Cluster type k8s or ocp production.bundle Registry and organization for bundles registry and organization production.index Registry, organization and for temporary index registry , organization and name production.index.tags List of index tags latest for k8s and v4.8-db, v4.9-db, v4.10-db, v4.11, v4.12 for ocp production.index.signature Alias and endpoint to sign index image enabled , alias and endpoint production.mirror Production index setup enabled , registry , organization and name production.mirror.base From image used for index enabled , registry , organization and name production.mirror.base_tags List of tags used for from index. (When not exist most recent used) v4.5 ... v4.11 production.mirror.postfix Type of index used for production tag index: <empty> or sha index: s production.mirror.username.in Credential username used for accessing registry.redhat.io production.mirror.username.out Credential username used for pushing to production registry production.test.installation_skip List of projects that installation test will be skipped multiple operator names (eg. ack- is for operators starting with ack- ) Pipeline pipeline: base: https://github.com repo: k8s-operatorhub/community-operators branch: main ci_scripts_dir: https://raw.githubusercontent.com/redhat-openshift-ecosystem/community-operators-pipeline/ci/latest/ci/scripts image: quay.io/operator_testing/operator-test-playbooks:latest playbooks: repo: https://github.com/redhat-openshift-ecosystem/operator-test-playbooks branch: \"upstream-community\" automerge: enabled: 1 reviewers: enabled: 1 notifications: slack: 1 package_manifest_disabled: 0 Name Description Possble options pipeline.base GitHub project base https://github.com pipeline.repo GitHub project org/repo eg. k8s-operatorhub/community-operators pipeline.branch GitHub project branch used for operators eg. main pipeline.ci_scripts_dir Location of framework ci scripts using ci/latest branch pipeline.image Image used for container that tests and release quay.io/operator_testing/operator-test-playbooks:latest or quay.io/operator_testing/operator-test-playbooks:dev pipeline.playbooks Playbook repo used as default repo and branch pipeline.automerge Flag if automerge is enabled enabled pipeline.reviewers Flag if reviewers is enabled enabled pipeline.notifications Flag if notifications is enabled slack pipeline.package_manifest_disabled Flag if package manifest format is disabled or enabled/supported 0 or 1 Operatorhubio operatorhubio: enabled: 1 registry: quay.io organization: operator-framework name: upstream-community-operators tag: latest Name Description Value production.operatorhubio Index image used to generate operatorhub.io page enabled , registry , organization , name and tag","title":"Config"},{"location":"project/config/#configuration","text":"The configuration file has to be stored in $PROJECT_DIR/ci/pipeline-config.yaml . There are three main sections production pipeline operatorhubio (for k8s ) Supported config options are explained for all sections below","title":"Configuration"},{"location":"project/config/#production","text":"production: type: ocp bundle: registry: quay.io organization: openshift-community-operators index: registry: quay.io organization: openshift-community-operators name: catalog_tmp tags: - v4.8-db - v4.9-db - v4.10-db - v4.11 - v4.12 signature: enabled: 1 alias: registry.redhat.io/redhat/community-operator-index endpoint: https://community-signing-pipeline-prod.apps.pipelines-prod.ijdb.p1.openshiftapps.com mirror: enabled: 1 multiarch: base: registry.redhat.io/openshift4/ose-operator-registry base_tags: - v4.5 - v4.6 - v4.7 - v4.8 - v4.9 - v4.10 - v4.11 postfix: s registry: quay.io organization: redhat name: redhat----community-operator-index username: in: mavala out: redhat+iib_community test: installation_skip: - ack- - community-windows-machine-config-operator Name Description Possible options production.type Cluster type k8s or ocp production.bundle Registry and organization for bundles registry and organization production.index Registry, organization and for temporary index registry , organization and name production.index.tags List of index tags latest for k8s and v4.8-db, v4.9-db, v4.10-db, v4.11, v4.12 for ocp production.index.signature Alias and endpoint to sign index image enabled , alias and endpoint production.mirror Production index setup enabled , registry , organization and name production.mirror.base From image used for index enabled , registry , organization and name production.mirror.base_tags List of tags used for from index. (When not exist most recent used) v4.5 ... v4.11 production.mirror.postfix Type of index used for production tag index: <empty> or sha index: s production.mirror.username.in Credential username used for accessing registry.redhat.io production.mirror.username.out Credential username used for pushing to production registry production.test.installation_skip List of projects that installation test will be skipped multiple operator names (eg. ack- is for operators starting with ack- )","title":"Production"},{"location":"project/config/#pipeline","text":"pipeline: base: https://github.com repo: k8s-operatorhub/community-operators branch: main ci_scripts_dir: https://raw.githubusercontent.com/redhat-openshift-ecosystem/community-operators-pipeline/ci/latest/ci/scripts image: quay.io/operator_testing/operator-test-playbooks:latest playbooks: repo: https://github.com/redhat-openshift-ecosystem/operator-test-playbooks branch: \"upstream-community\" automerge: enabled: 1 reviewers: enabled: 1 notifications: slack: 1 package_manifest_disabled: 0 Name Description Possble options pipeline.base GitHub project base https://github.com pipeline.repo GitHub project org/repo eg. k8s-operatorhub/community-operators pipeline.branch GitHub project branch used for operators eg. main pipeline.ci_scripts_dir Location of framework ci scripts using ci/latest branch pipeline.image Image used for container that tests and release quay.io/operator_testing/operator-test-playbooks:latest or quay.io/operator_testing/operator-test-playbooks:dev pipeline.playbooks Playbook repo used as default repo and branch pipeline.automerge Flag if automerge is enabled enabled pipeline.reviewers Flag if reviewers is enabled enabled pipeline.notifications Flag if notifications is enabled slack pipeline.package_manifest_disabled Flag if package manifest format is disabled or enabled/supported 0 or 1","title":"Pipeline"},{"location":"project/config/#operatorhubio","text":"operatorhubio: enabled: 1 registry: quay.io organization: operator-framework name: upstream-community-operators tag: latest Name Description Value production.operatorhubio Index image used to generate operatorhub.io page enabled , registry , organization , name and tag","title":"Operatorhubio"},{"location":"project/develop/","text":"Development Develop a new feature or fix a bug Initialize an own branch from dev branch Develop a feature or bugfix itself Open a PR against dev branch. Trigger an image build when playbooks were edited or upgrade testing environment when workflows were edited Test the playbook functionality Create a PR against the production branch Wait until an image is produced automatically in case of playbook changes Upgrade production repositories when workflow templates were changed We will provide more details about some steps below: Starting branch Playbook case To develop a new feature, a developer has to start from upstream-community-dev branch in playbooks by creating their own branch. Make sure that the development branch upstream-community-dev is synchronized with production one upstream-community . Workflow case The development branch which is your base branch is ci/dev . Please make sure that the development branch ci/dev is synchronized with the production branch ci/latest . Never edit production workflows directly, which are located in .github/workflows directory. Your changes will be lost during the next pipeline upgrade - during the next feature implementation. PR to dev branch Playbook case When a feature/bug fix is ready, please open a PR against upstream-community-dev branch. There aren't any valid unit tests, just obsolete ones. You can ignore failure there. The only valid test is linting, which should pass. Workflow case Open a PR against ci/dev branch. Image build trigger Playbook case To build an image from just edited playbooks, trigger it by GH action called Build playbook image Click Run workflow on the top right and select a branch. In this case upstream-community-dev . The functionality allows triggering image build from the feature branch also, however, the naming convention should be like upstream-community-dev-[something] . In both cases ( upstream-community-dev-[something] and upstream-community-dev ) your action will produce quay.io/operator_testing/operator-test-playbooks:dev image. Workflow case - repo upgrade No need to build any image. To populate edited workflow templates to the testing project, please execute Upgrade GH action . Set branches which you want to test. The standard setup is ci-dev and upstream-community-dev . Test a playbook funkcionality To test a currently implemented feature, one or more test cases need to be set. The only test which is expected to fail is ci/prow/4.8-pipeline-functionality . There is a space for improvement to make the test work as a quality gate for Prow job workflow file changes. Back to tests, we should focus on. We will test the following parts of the test suite: - Basic PR tests - Temp index test - Prow test - Release test Basic PR tests To execute all basic PR tests, please edit a description in an operator in the test pipeline environment . Please do not add more operators if not necessary, it can increase the repository size for workflow templates. We recommend editing just a description in an operator. Push to your branch on your own GH repository. Then open a PR. This corresponds with the setup of how contributors work against the community repositories. Prow test Every PR triggers a Prow test no matter if it is K8S or OCP. This is a standard setup only for community-operators-pipeline pipeline. Production pipelines run Prow only if needed. More information on where to find the Prow debug log is here . Temp index test When prow fails it can be due to a missing temporary index or temporary bundle. Temp index and the bundle are processed by Prepare Test Index workflow. Logs are directly available. If you are not sure to which PR is workflow run related, check Build temp index and push stage. You should see something like Preparing temp index for PR: 347 . Release test When all tests are green, expect ci/prow/4.8-pipeline-functionality as mentioned above, we can test release by merging the PR. Then a release can be checked directly here , we expect a green result. Create a PR against the production branch Now you are in the phase to create PRs from dev to production for both playbooks and framework: Open and merge community-operators-pipeline:ci/dev to community-operators-pipeline:ci/latest . Do not apply it by Upgrade procedure next steps are completed to the state that a new playbook image is out. Open a PR from operator-test-playbooks:upstream-community-dev to operator-test-playbooks:upstream-community . Wait until an image is produced automatically in case of playbook changes After it is merged, just wait until Build playbook image pipeline has finished. Upgrade production repositories when workflow templates were changed Now, it is time to apply workflows by running CI Upgrade on production OCP and k8s production . Failure is not bad if workflows should stay as it is, it just means nothing was changed. If there should be changes, investigate why no change is present. Add a feature with an automatic comment to a PR In some cases, it is very useful to notify a contributor directly to a PR thread. We are using github.issues.createComment action. It is used many times, one example bellow: with : github-token : ${{secrets.GITHUB_TOKEN}} script : | github.issues.createComment({ issue_number: context.issue.number, owner: context.repo.owner, repo: context.repo.repo, body: 'Current PR can be merged automatically, but there is missing `authorized-changes` label. One can find out more info [here](https://${OPP_THIS_REPO_ORG}.github.io/${OPP_THIS_REPO_NAME}/operator-ci-yaml/#reviewers).' })","title":"How to"},{"location":"project/develop/#development","text":"","title":"Development"},{"location":"project/develop/#develop-a-new-feature-or-fix-a-bug","text":"Initialize an own branch from dev branch Develop a feature or bugfix itself Open a PR against dev branch. Trigger an image build when playbooks were edited or upgrade testing environment when workflows were edited Test the playbook functionality Create a PR against the production branch Wait until an image is produced automatically in case of playbook changes Upgrade production repositories when workflow templates were changed We will provide more details about some steps below:","title":"Develop a new feature or fix a bug"},{"location":"project/develop/#starting-branch","text":"","title":"Starting branch"},{"location":"project/develop/#pr-to-dev-branch","text":"","title":"PR to dev branch"},{"location":"project/develop/#image-build-trigger","text":"","title":"Image build trigger"},{"location":"project/develop/#test-a-playbook-funkcionality","text":"To test a currently implemented feature, one or more test cases need to be set. The only test which is expected to fail is ci/prow/4.8-pipeline-functionality . There is a space for improvement to make the test work as a quality gate for Prow job workflow file changes. Back to tests, we should focus on. We will test the following parts of the test suite: - Basic PR tests - Temp index test - Prow test - Release test","title":"Test a playbook funkcionality"},{"location":"project/develop/#release-test","text":"When all tests are green, expect ci/prow/4.8-pipeline-functionality as mentioned above, we can test release by merging the PR. Then a release can be checked directly here , we expect a green result.","title":"Release test"},{"location":"project/develop/#create-a-pr-against-the-production-branch","text":"Now you are in the phase to create PRs from dev to production for both playbooks and framework: Open and merge community-operators-pipeline:ci/dev to community-operators-pipeline:ci/latest . Do not apply it by Upgrade procedure next steps are completed to the state that a new playbook image is out. Open a PR from operator-test-playbooks:upstream-community-dev to operator-test-playbooks:upstream-community .","title":"Create a PR against the production branch"},{"location":"project/develop/#wait-until-an-image-is-produced-automatically-in-case-of-playbook-changes","text":"After it is merged, just wait until Build playbook image pipeline has finished.","title":"Wait until an image is produced automatically in case of playbook changes"},{"location":"project/develop/#upgrade-production-repositories-when-workflow-templates-were-changed","text":"Now, it is time to apply workflows by running CI Upgrade on production OCP and k8s production . Failure is not bad if workflows should stay as it is, it just means nothing was changed. If there should be changes, investigate why no change is present.","title":"Upgrade production repositories when workflow templates were changed"},{"location":"project/develop/#add-a-feature-with-an-automatic-comment-to-a-pr","text":"In some cases, it is very useful to notify a contributor directly to a PR thread. We are using github.issues.createComment action. It is used many times, one example bellow: with : github-token : ${{secrets.GITHUB_TOKEN}} script : | github.issues.createComment({ issue_number: context.issue.number, owner: context.repo.owner, repo: context.repo.repo, body: 'Current PR can be merged automatically, but there is missing `authorized-changes` label. One can find out more info [here](https://${OPP_THIS_REPO_ORG}.github.io/${OPP_THIS_REPO_NAME}/operator-ci-yaml/#reviewers).' })","title":"Add a feature with an automatic comment to a PR"},{"location":"project/init/","text":"Initialize new project Create empty project The project administrator has to create an empty GitHub project configure one configuration file set up various secrets create the directory structure shown here","title":"Initialize"},{"location":"project/init/#initialize-new-project","text":"","title":"Initialize new project"},{"location":"project/init/#create-empty-project","text":"The project administrator has to create an empty GitHub project configure one configuration file set up various secrets create the directory structure shown here","title":"Create empty project"},{"location":"project/k8s/","text":"Project maintain Overview The pipeline tests are using kind cluster to test operator installation with local container registry setup. Production bundle and catalog locations Type Image Description Bundles quay.io/operatorhubio/<operator-name>:v<operator-version> Example: quay.io/operatorhubio/etcd:v0.9.4 Temporary index (tags) quay.io/operatorhubio/catalog_tmp:latest Index contains packages with version same as bundle tag name Temporary index (shas) quay.io/operatorhubio/catalog_tmp:latests Index contains packages with version as bundle sha (used for production) Production index quay.io/operatorhubio/catalog:latest Multiarch production index image used in operatorhubio-catalog seen by OLM Kind cluster and Kubernetes version By default, the latest k8s version supported by kind is used until the user doesn't limit it via operatorhub.io/ui-metadata-max-k8s-version defined in metadata.annotations in the CSV file. In case the following value is set operatorhub.io/ui-metadata-max-k8s-version: \"1.21\" then the kind cluster will use Kubernetes latest 1.21 version that kind supports. One can list available Kubernetes versions in kind project release section","title":"Kubernetes"},{"location":"project/k8s/#project-maintain","text":"","title":"Project maintain"},{"location":"project/k8s/#overview","text":"The pipeline tests are using kind cluster to test operator installation with local container registry setup.","title":"Overview"},{"location":"project/k8s/#production-bundle-and-catalog-locations","text":"Type Image Description Bundles quay.io/operatorhubio/<operator-name>:v<operator-version> Example: quay.io/operatorhubio/etcd:v0.9.4 Temporary index (tags) quay.io/operatorhubio/catalog_tmp:latest Index contains packages with version same as bundle tag name Temporary index (shas) quay.io/operatorhubio/catalog_tmp:latests Index contains packages with version as bundle sha (used for production) Production index quay.io/operatorhubio/catalog:latest Multiarch production index image used in operatorhubio-catalog seen by OLM","title":"Production bundle and catalog locations"},{"location":"project/k8s/#kind-cluster-and-kubernetes-version","text":"By default, the latest k8s version supported by kind is used until the user doesn't limit it via operatorhub.io/ui-metadata-max-k8s-version defined in metadata.annotations in the CSV file. In case the following value is set operatorhub.io/ui-metadata-max-k8s-version: \"1.21\" then the kind cluster will use Kubernetes latest 1.21 version that kind supports. One can list available Kubernetes versions in kind project release section","title":"Kind cluster and Kubernetes version"},{"location":"project/maintain/","text":"Maintainance Labels The operator project is using various labels to handle different situations. Here is the list of them and their meaning Name Description allow/operator-version-overwrite Operator version will be overritten (only cosmetics changes) allow/serious-changes-to-existing User is overwiting field in csv that are not allowed by default via previous label. Maintainer can allow these changes to be applied also. allow/operator-recreate Operator will be recreated (deleted/created). It happens when mutiple versions are modified of operator allow/ci-changes When there are changes outside of operators directory maintainer can set this label to skip failing allow/longer-deployment It sets longer time for operator installation timeout authorized-changes The changes are autorized. User can have this automatically when author is in reviever list in the ci.yaml file(needs for automerge) dco-failed DCO failed. Commits were not signedoff package-validated Package is validated (needs for automerge) installation-validated Installation is validated (needs for automerge) installation-skipped Installation is skipped. Some operators requested not to test installation needs-rebase User should rebase to latest main branch new-operator Label if operator is new. In other words if there is only one version of operator Upgrade GitHub Action - CI Upgrade On every workflow template and config change, one has to run an upgrade to apply changes for each project. Name Description Commit message prefix Prefix added to commit message after upgrade Source repository Framework (workflow templates) project ( https://github.com/redhat-openshift-ecosystem/community-operators-pipeline ) Source branch Framework (workflow templates) branch ( ci/latest ) Playbook branch Branch ( upstream-community ) in ansible playbooks are taken to upgrade Cluster type (k8s or ocp) Cluster type for repo. Possible options k8s or ocp From index (quay.io/operator_testing/index_empty:latest) Optional parameter to initialize or copy index image to nonexisten images For https://github.com/redhat-openshift-ecosystem/community-operators-pipeline repository, there is a different IIB_INPUT_REGISTRY_TOKEN in k8s upgrade than in ocp upgrade. Change the token during the upgrade accordingly. Do not change any token in production, just on the development pipeline. GitHub Action - Operator convert The goal of Operator convert workflow is to convert the package manifest format operator to a bundle. It is done in two steps Convert operators to target branch Make PR from target branch to main One can run multiple operators in one go, but be aware that the conversion process of the operator that has many versions might take a few hours. It is recommended to run more operators with a small number of versions first and then operators with many versions. Disable package manifest format support Since this process can take a long time, it is recommended to convert operators before the deadline (31. June 2023). Once the conversion process is finished and all operators are converted, one can disable the package manifest format by setting the configuration pipeline package_manifest_disabled value to 1 and run CI Upgrade explained above. Package manifest format conversion workflow The whole process can be run via the following workflow Name Description List of operators divided by space (aqua dell-csi-operator). Value all is also supported List of operators. Option all will search for all opeators that are in package manifest format Target branch Target branch where convertion of operator is pushed. One can make PR to man after process is finished. Note that branch is overwritten automatically by running next workflow if target branch is same. Playbook repo Playbook repo ( https://github.com/redhat-openshift-ecosystem/operator-test-playbooks ) Playbook branch Branch ( upstream-community ) in ansible playbooks","title":"Common"},{"location":"project/maintain/#maintainance","text":"","title":"Maintainance"},{"location":"project/maintain/#labels","text":"The operator project is using various labels to handle different situations. Here is the list of them and their meaning Name Description allow/operator-version-overwrite Operator version will be overritten (only cosmetics changes) allow/serious-changes-to-existing User is overwiting field in csv that are not allowed by default via previous label. Maintainer can allow these changes to be applied also. allow/operator-recreate Operator will be recreated (deleted/created). It happens when mutiple versions are modified of operator allow/ci-changes When there are changes outside of operators directory maintainer can set this label to skip failing allow/longer-deployment It sets longer time for operator installation timeout authorized-changes The changes are autorized. User can have this automatically when author is in reviever list in the ci.yaml file(needs for automerge) dco-failed DCO failed. Commits were not signedoff package-validated Package is validated (needs for automerge) installation-validated Installation is validated (needs for automerge) installation-skipped Installation is skipped. Some operators requested not to test installation needs-rebase User should rebase to latest main branch new-operator Label if operator is new. In other words if there is only one version of operator","title":"Labels"},{"location":"project/maintain/#upgrade","text":"","title":"Upgrade"},{"location":"project/maintain/#github-action-ci-upgrade","text":"On every workflow template and config change, one has to run an upgrade to apply changes for each project. Name Description Commit message prefix Prefix added to commit message after upgrade Source repository Framework (workflow templates) project ( https://github.com/redhat-openshift-ecosystem/community-operators-pipeline ) Source branch Framework (workflow templates) branch ( ci/latest ) Playbook branch Branch ( upstream-community ) in ansible playbooks are taken to upgrade Cluster type (k8s or ocp) Cluster type for repo. Possible options k8s or ocp From index (quay.io/operator_testing/index_empty:latest) Optional parameter to initialize or copy index image to nonexisten images For https://github.com/redhat-openshift-ecosystem/community-operators-pipeline repository, there is a different IIB_INPUT_REGISTRY_TOKEN in k8s upgrade than in ocp upgrade. Change the token during the upgrade accordingly. Do not change any token in production, just on the development pipeline.","title":"GitHub Action - CI Upgrade"},{"location":"project/maintain/#github-action-operator-convert","text":"The goal of Operator convert workflow is to convert the package manifest format operator to a bundle. It is done in two steps Convert operators to target branch Make PR from target branch to main One can run multiple operators in one go, but be aware that the conversion process of the operator that has many versions might take a few hours. It is recommended to run more operators with a small number of versions first and then operators with many versions.","title":"GitHub Action - Operator convert"},{"location":"project/ocp/","text":"Red Hat OpenShift Container Platform (OCP) The following part is related to Openshift only. Production bundle and catalog locations Type Image Description Bundles quay.io/openshift-community-operators/<operator-name>:v<operator-version> Example: quay.io/openshift-community-operators/etcd:v0.9.4 Temporary index (tags) quay.io/openshift-community-operators/catalog_tmp:v<ocp-version> Index contains packages with version same as bundle tag name. Example for ocp v4.11: quay.io/openshift-community-operators/catalog_tmp:v4.11 Temporary index (shas) quay.io/openshift-community-operators/catalog_tmp:v<ocp-version>s Index contains packages with version as bundle sha (used for production). Example for ocp v4.11: quay.io/openshift-community-operators/catalog_tmp:v4.11 Pre-Production index quay.io/openshift-community-operators/catalog:v<ocp-version> Multiarch production index image used in OCP cluster. Example for ocp v4.11: quay.io/openshift-community-operators/catalog:v4.11 Production index quay.io/openshift-community-operators/catalog:v<ocp-version> Multiarch production index image used in OCP cluster. Example for ocp v4.11: quay.io/openshift-community-operators/catalog:v4.11 OCP max OpenShift version In the bundle format, one can set OCP version range by adding com.redhat.openshift.versions: \"v4.8-v4.11\" to metadata/annotations.yaml file (see below). annotations: # Core bundle annotations. operators.operatorframework.io.bundle.mediatype.v1: registry+v1 ... ... com.redhat.openshift.versions: \"v4.8-v4.11\" For package manifest format it is not possible, but there is an option to set the maximum ocp version via csv in metadata.annotations key. One can add the following olm.properties: '[{\"type\": \"olm.maxOpenShiftVersion\", \"value\": \"4.8\"}]'`(see below). apiVersion: operators.coreos.com/v1alpha1 kind: ClusterServiceVersion metadata: annotations: # Setting olm.maxOpenShiftVersion automatically # This property was added via an automatic process since it was possible to identify that this distribution uses API(s), # which will be removed in the k8s version 1.22 and OpenShift version OCP 4.9. Then, it will prevent OCP users to # upgrade their cluster to 4.9 before they have installed in their current clusters a version of your operator that # is compatible with it. Please, ensure that your project is no longer using these API(s) and that you start to # distribute solutions which is compatible with Openshift 4.9. # For further information, check the README of this repository. olm.properties: '[{\"type\": \"olm.maxOpenShiftVersion\", \"value\": \"4.8\"}]' ... How OCP installation is tested Prow is an external OpenShift release tooling framework that is used as an installation test in the community pipeline. How to edit prow building block configuration Prow is configured at openshift repository . Open a PR and get LGTM approval from your colleague to get an automatic merge. In case you are creating a new project, make sure openshift-ci-robot is added as a collaborator to the project with Admin rights. Overview The prow job is automatically triggered for every OCP PR if GH Action did not fail at the beginning. See the structure below. graph TD openshift-deploy.sh --> openshift-deploy-core.sh --> waiting(\"wait for hash label on Quay\") --> deploy_olm_operator_openshift_upstream openshift-deploy-core.sh --> prepare_test_index prepare_test_index -.-> Quay Quay -.-> deploy_olm_operator_openshift_upstream style Quay stroke-dasharray: 5 5 subgraph prow openshift-deploy.sh & openshift-deploy-core.sh & waiting subgraph Ansible role deploy_olm_operator_openshift_upstream end end subgraph GH Action job prepare_test_index & Quay end Openshift robot triggers cluster setup for every supported OCP version. When the cluster is ready, openshift-deploy.sh is executed. The script calls another script openshift-deploy-core.sh which triggers GH Action prepare_test_index . During the action run, it pushes the index and a bundle to Quay tagged by a commit hash. Once images are pushed, the playbook role deploy_olm_operator_openshift_upstream is triggered which pulls the images and installs the operator. Where to edit the main openshift script To edit openshift-deploy.sh located in ci/prow of the project, first edit openshift-deploy.sh located in CI repository. Then upgrade the project running Upgrade CI . The same applies for openshift-deploy-core.sh . Consider using ci/dev instead of ci/latest during development as described here . Where to edit deploy_olm_operator_openshift_upstream role Like every Ansible role, editing is possible in upstream directory of ansible playbook repository . When using the production branch upstream-community , automatic playbook image build is triggered. When using the development branch upstream-community-dev , please trigger playbook image build manually as described here . Consider using upstream-community-dev instead of upstream-community during development as described here . Where to edit or restart prepare_test_index action To restart prepare_test_index action, go to GH Actions of the project. When an edit is needed, go to templates . Consider using ci/dev instead of ci/latest during development as described here . Release brand new index for OCP Let's assume we are going to release the index for OCP v4.13 . Prerequisities Before running an automatic GH action that creates indexes itself, there are some prerequisites administrator should prepare in a specific order: Add new index mapping Enable Pyxis support for a specific index Set maximum oc version available OCP and K8S alignment Enable breaking API testing if supported by operator-sdk When all done, bump ocp_version_example variable so next time examples are up to date :) Add new index mapping Always check and add the current index (e.g. v4.13 ) version to operator_info role defaults and to k8s2ocp and ocp2k8s converting tables in bundle_validation_filter OCP2K8S and KIND_SUPPORT_TABLE variable in ci/dev and ci/latest consequently Enable Pyxis support for a new index To enable pyxis support for a specific index, clone the issue . And update index number (e.g. v4.13 ) in the description. Set maximum oc version available Edit oc_version_max in playbook defaults only if 4.x (e.g. v4.13 ) is available at https://mirror.openshift.com/pub/openshift-v4/clients/ocp/latest-4.x/openshift-client-linux.tar.gz (e.g. https://mirror.openshift.com/pub/openshift-v4/clients/ocp/latest-4.13/openshift-client-linux.tar.gz) OCP and K8S alignment Despite this documentation being focused on OCP, alignment with k8s is needed on community-operators k8s-operatorhub repository. Firstly, set kind_version to the latest kind according to https://github.com/kubernetes-sigs/kind/releases (e.g. v0.17.0). Also the same page contains semver version of a specific k8s image. So for 1.25 we are reading v1.25.3 . Set the value as kind_kube_version . Enable breaking API testing if supported by operator-sdk If there is some breaking API in a new index (e.g. v4.13 ), please edit bundle_validation_filter role defaults to enable testing if API is broken in a specific operator. Release process Firstly, the index must be defined in pipeline-config-ocp.yaml file. There are old entries like v4.10-db where -db means index is in SQLlite format. It is just for the information, not important here. A new entry can be one of the following: - v4.13-maintenance - release the specific index will not be executed, kiwi lemon orange tests are always green, failed Prow is not blocking merge action - v4.13-rc - release the specific index will be executed, kiwi lemon orange tests are always green, failed Prow is not blocking merge action - v4.13 - full production setup, needs all tests green before merge action Admins are asked to provide a new Openshift index a couple of months before a new Openshift version is GA. There are 2 ways of releasing a new index. The very first step is to have the entry in pipeline-config-ocp.yaml like in the example: - v4.13-maintenance . This is a label for the target index in case of a new index release. Release from a previous index This is a recommended way. Much faster and easier to execute. Everything is managed by the automatic workflow called CI Upgrade . Fill fields as shown below. The most important field is From index . There should be a path directly to a previous _tmp image. Use path like quay.io/openshift-community-operators/catalog_tmp:v4.12 if you would like to release v4.13 . When the workflow is finished, see the list of operators to fix in the new index. The list is located on the GH workflow output page as Upgrade summary . The example Upgrade pipeline is located here . Create local changes step in upgrade job does the whole process. The log is located here Then you need to fix operators by running Operator release manual . Set values as in the example below. The most important field is the List of operators ... - it is a place to put the output from the previous workflow under the Upgrade summary . The list is already space delimited. The example Manual release pipeline schema is located here and the example output with steps here ](https://github.com/redhat-openshift-ecosystem/community-operators-prod/actions/runs/3740100606/jobs/6349116153){:target=\"_blank\"}. How to rebuild an existing index from scratch There can be cases when differences between an actual and a new index are huge. In this case, it makes sense to fill the new index from scratch. You need only Operator release manual . Be ready for a day or more and multiple manual triggers of the same workflow type with a different set of operators. This time, a List of operators... is a list of all operators in the GitHub repository divided into chunks that can be processed in 6 hours or less each. Hence GH actions limit. The best practice is to use 1/5th of the full operator list divided by a space. A release process in this case is long this way so use it as a last resort. It can be partially optimized by running over operators sorted by the number of versions inside a package. It helps the parallel process to finish smaller operators sooner. Do not enable Push final index to production until all operators are processed. Or you can always leave the value 0 and the next automatic merge will push also your changes to production. Release process is expected to fail at the end due to the fact, that index is not fully synchronized until all operators are processed. It is OK.","title":"Openshift (OCP)"},{"location":"project/ocp/#red-hat-openshift-container-platform-ocp","text":"The following part is related to Openshift only.","title":"Red Hat OpenShift Container Platform (OCP)"},{"location":"project/ocp/#production-bundle-and-catalog-locations","text":"Type Image Description Bundles quay.io/openshift-community-operators/<operator-name>:v<operator-version> Example: quay.io/openshift-community-operators/etcd:v0.9.4 Temporary index (tags) quay.io/openshift-community-operators/catalog_tmp:v<ocp-version> Index contains packages with version same as bundle tag name. Example for ocp v4.11: quay.io/openshift-community-operators/catalog_tmp:v4.11 Temporary index (shas) quay.io/openshift-community-operators/catalog_tmp:v<ocp-version>s Index contains packages with version as bundle sha (used for production). Example for ocp v4.11: quay.io/openshift-community-operators/catalog_tmp:v4.11 Pre-Production index quay.io/openshift-community-operators/catalog:v<ocp-version> Multiarch production index image used in OCP cluster. Example for ocp v4.11: quay.io/openshift-community-operators/catalog:v4.11 Production index quay.io/openshift-community-operators/catalog:v<ocp-version> Multiarch production index image used in OCP cluster. Example for ocp v4.11: quay.io/openshift-community-operators/catalog:v4.11","title":"Production bundle and catalog locations"},{"location":"project/ocp/#ocp-max-openshift-version","text":"In the bundle format, one can set OCP version range by adding com.redhat.openshift.versions: \"v4.8-v4.11\" to metadata/annotations.yaml file (see below). annotations: # Core bundle annotations. operators.operatorframework.io.bundle.mediatype.v1: registry+v1 ... ... com.redhat.openshift.versions: \"v4.8-v4.11\" For package manifest format it is not possible, but there is an option to set the maximum ocp version via csv in metadata.annotations key. One can add the following olm.properties: '[{\"type\": \"olm.maxOpenShiftVersion\", \"value\": \"4.8\"}]'`(see below). apiVersion: operators.coreos.com/v1alpha1 kind: ClusterServiceVersion metadata: annotations: # Setting olm.maxOpenShiftVersion automatically # This property was added via an automatic process since it was possible to identify that this distribution uses API(s), # which will be removed in the k8s version 1.22 and OpenShift version OCP 4.9. Then, it will prevent OCP users to # upgrade their cluster to 4.9 before they have installed in their current clusters a version of your operator that # is compatible with it. Please, ensure that your project is no longer using these API(s) and that you start to # distribute solutions which is compatible with Openshift 4.9. # For further information, check the README of this repository. olm.properties: '[{\"type\": \"olm.maxOpenShiftVersion\", \"value\": \"4.8\"}]' ...","title":"OCP max OpenShift version"},{"location":"project/ocp/#how-ocp-installation-is-tested","text":"Prow is an external OpenShift release tooling framework that is used as an installation test in the community pipeline.","title":"How OCP installation is tested"},{"location":"project/ocp/#how-to-edit-prow-building-block-configuration","text":"Prow is configured at openshift repository . Open a PR and get LGTM approval from your colleague to get an automatic merge. In case you are creating a new project, make sure openshift-ci-robot is added as a collaborator to the project with Admin rights.","title":"How to edit prow building block configuration"},{"location":"project/ocp/#overview","text":"The prow job is automatically triggered for every OCP PR if GH Action did not fail at the beginning. See the structure below. graph TD openshift-deploy.sh --> openshift-deploy-core.sh --> waiting(\"wait for hash label on Quay\") --> deploy_olm_operator_openshift_upstream openshift-deploy-core.sh --> prepare_test_index prepare_test_index -.-> Quay Quay -.-> deploy_olm_operator_openshift_upstream style Quay stroke-dasharray: 5 5 subgraph prow openshift-deploy.sh & openshift-deploy-core.sh & waiting subgraph Ansible role deploy_olm_operator_openshift_upstream end end subgraph GH Action job prepare_test_index & Quay end Openshift robot triggers cluster setup for every supported OCP version. When the cluster is ready, openshift-deploy.sh is executed. The script calls another script openshift-deploy-core.sh which triggers GH Action prepare_test_index . During the action run, it pushes the index and a bundle to Quay tagged by a commit hash. Once images are pushed, the playbook role deploy_olm_operator_openshift_upstream is triggered which pulls the images and installs the operator.","title":"Overview"},{"location":"project/ocp/#where-to-edit-the-main-openshift-script","text":"To edit openshift-deploy.sh located in ci/prow of the project, first edit openshift-deploy.sh located in CI repository. Then upgrade the project running Upgrade CI . The same applies for openshift-deploy-core.sh . Consider using ci/dev instead of ci/latest during development as described here .","title":"Where to edit the main openshift script"},{"location":"project/ocp/#where-to-edit-deploy_olm_operator_openshift_upstream-role","text":"Like every Ansible role, editing is possible in upstream directory of ansible playbook repository . When using the production branch upstream-community , automatic playbook image build is triggered. When using the development branch upstream-community-dev , please trigger playbook image build manually as described here . Consider using upstream-community-dev instead of upstream-community during development as described here .","title":"Where to edit deploy_olm_operator_openshift_upstream role"},{"location":"project/ocp/#where-to-edit-or-restart-prepare_test_index-action","text":"To restart prepare_test_index action, go to GH Actions of the project. When an edit is needed, go to templates . Consider using ci/dev instead of ci/latest during development as described here .","title":"Where to edit or restart prepare_test_index action"},{"location":"project/ocp/#release-brand-new-index-for-ocp","text":"Let's assume we are going to release the index for OCP v4.13 .","title":"Release brand new index for OCP"},{"location":"project/ocp/#prerequisities","text":"Before running an automatic GH action that creates indexes itself, there are some prerequisites administrator should prepare in a specific order: Add new index mapping Enable Pyxis support for a specific index Set maximum oc version available OCP and K8S alignment Enable breaking API testing if supported by operator-sdk When all done, bump ocp_version_example variable so next time examples are up to date :)","title":"Prerequisities"},{"location":"project/ocp/#add-new-index-mapping","text":"Always check and add the current index (e.g. v4.13 ) version to operator_info role defaults and to k8s2ocp and ocp2k8s converting tables in bundle_validation_filter OCP2K8S and KIND_SUPPORT_TABLE variable in ci/dev and ci/latest consequently","title":"Add new index mapping"},{"location":"project/ocp/#enable-pyxis-support-for-a-new-index","text":"To enable pyxis support for a specific index, clone the issue . And update index number (e.g. v4.13 ) in the description.","title":"Enable Pyxis support for a new index"},{"location":"project/ocp/#set-maximum-oc-version-available","text":"Edit oc_version_max in playbook defaults only if 4.x (e.g. v4.13 ) is available at https://mirror.openshift.com/pub/openshift-v4/clients/ocp/latest-4.x/openshift-client-linux.tar.gz (e.g. https://mirror.openshift.com/pub/openshift-v4/clients/ocp/latest-4.13/openshift-client-linux.tar.gz)","title":"Set maximum oc version available"},{"location":"project/ocp/#ocp-and-k8s-alignment","text":"Despite this documentation being focused on OCP, alignment with k8s is needed on community-operators k8s-operatorhub repository. Firstly, set kind_version to the latest kind according to https://github.com/kubernetes-sigs/kind/releases (e.g. v0.17.0). Also the same page contains semver version of a specific k8s image. So for 1.25 we are reading v1.25.3 . Set the value as kind_kube_version .","title":"OCP and K8S alignment"},{"location":"project/ocp/#release-process","text":"Firstly, the index must be defined in pipeline-config-ocp.yaml file. There are old entries like v4.10-db where -db means index is in SQLlite format. It is just for the information, not important here. A new entry can be one of the following: - v4.13-maintenance - release the specific index will not be executed, kiwi lemon orange tests are always green, failed Prow is not blocking merge action - v4.13-rc - release the specific index will be executed, kiwi lemon orange tests are always green, failed Prow is not blocking merge action - v4.13 - full production setup, needs all tests green before merge action Admins are asked to provide a new Openshift index a couple of months before a new Openshift version is GA. There are 2 ways of releasing a new index. The very first step is to have the entry in pipeline-config-ocp.yaml like in the example: - v4.13-maintenance . This is a label for the target index in case of a new index release.","title":"Release process"},{"location":"project/ocp/#release-from-a-previous-index","text":"This is a recommended way. Much faster and easier to execute. Everything is managed by the automatic workflow called CI Upgrade . Fill fields as shown below. The most important field is From index . There should be a path directly to a previous _tmp image. Use path like quay.io/openshift-community-operators/catalog_tmp:v4.12 if you would like to release v4.13 . When the workflow is finished, see the list of operators to fix in the new index. The list is located on the GH workflow output page as Upgrade summary . The example Upgrade pipeline is located here . Create local changes step in upgrade job does the whole process. The log is located here Then you need to fix operators by running Operator release manual . Set values as in the example below. The most important field is the List of operators ... - it is a place to put the output from the previous workflow under the Upgrade summary . The list is already space delimited. The example Manual release pipeline schema is located here and the example output with steps here ](https://github.com/redhat-openshift-ecosystem/community-operators-prod/actions/runs/3740100606/jobs/6349116153){:target=\"_blank\"}.","title":"Release from a previous index"},{"location":"project/ocp/#how-to-rebuild-an-existing-index-from-scratch","text":"There can be cases when differences between an actual and a new index are huge. In this case, it makes sense to fill the new index from scratch. You need only Operator release manual . Be ready for a day or more and multiple manual triggers of the same workflow type with a different set of operators. This time, a List of operators... is a list of all operators in the GitHub repository divided into chunks that can be processed in 6 hours or less each. Hence GH actions limit. The best practice is to use 1/5th of the full operator list divided by a space. A release process in this case is long this way so use it as a last resort. It can be partially optimized by running over operators sorted by the number of versions inside a package. It helps the parallel process to finish smaller operators sooner. Do not enable Push final index to production until all operators are processed. Or you can always leave the value 0 and the next automatic merge will push also your changes to production. Release process is expected to fail at the end due to the fact, that index is not fully synchronized until all operators are processed. It is OK.","title":"How to rebuild an existing index from scratch"},{"location":"project/overview/","text":"Project Overview The goal of the project is to test, release and deploy an operator to index so it is easily installable. The process is displayed below. graph TD id1(Operator locally) --> id2(PR to project) --> id3(Operator test) --> id4(Operator release to index)--> id5(Install operator on cluster) Let's get started with Project initialize .","title":"Overview"},{"location":"project/overview/#project-overview","text":"The goal of the project is to test, release and deploy an operator to index so it is easily installable. The process is displayed below. graph TD id1(Operator locally) --> id2(PR to project) --> id3(Operator test) --> id4(Operator release to index)--> id5(Install operator on cluster) Let's get started with Project initialize .","title":"Project Overview"},{"location":"project/setup/","text":"Secrets Screenshot of GitHub action secrets The table is explaining Name Description ACTION_MONITORING_SLACK Slack webhook credentials FRAMEWORK_MERGE GitHub action token for framework-automation user to be able to automerge GH_TOKEN GitHub action token for framework-automation to produce statistics IIB_INPUT_REGISTRY_TOKEN Token for user $IIB_INPUT_REGISTRY_USER to be able to access registry.redhat.io PREPARE_INDEX_API_TOKEN Quay application Token (user: $oauth ) used for preparing temporary index for prow jobs REGISTRY_MIRROR_PW Token for user $OPP_REGISTRY_MIRROR_USER to be able to push to production (mirror) index REGISTRY_RELEASE_API_TOKEN Quay application Token (user: $oauth ) used for pushing to release index REPO_GHA_PAT GitHub access token (deprecated) OHIO_REGISTRY_TOKEN Quay application token (user: $oauth ) to push index image used for operatorhub.io web page SIGNATURE_WEBHOOK_PASSWD Index signrature password SIGNATURE_WEBHOOK_REQUESTER_EMAIL Index signrature requester email SIGNATURE_WEBHOOK_SECRET Index signrature secret Generate Github Action workflows Note On newly created projects one should copy the file ( upgrade.yaml ) to .github/workflows/upgrade.yaml and push it into main branch so CI Upgrade workflow is enabled in the Actions tab in the GitHub project. After the directory and configurations are in place one can generate all workflows by running Upgrade Action from the configured project. More details one can find here One can verify Upgrade CI GitHub Action. See screenshot below","title":"Setup"},{"location":"project/setup/#secrets","text":"Screenshot of GitHub action secrets The table is explaining Name Description ACTION_MONITORING_SLACK Slack webhook credentials FRAMEWORK_MERGE GitHub action token for framework-automation user to be able to automerge GH_TOKEN GitHub action token for framework-automation to produce statistics IIB_INPUT_REGISTRY_TOKEN Token for user $IIB_INPUT_REGISTRY_USER to be able to access registry.redhat.io PREPARE_INDEX_API_TOKEN Quay application Token (user: $oauth ) used for preparing temporary index for prow jobs REGISTRY_MIRROR_PW Token for user $OPP_REGISTRY_MIRROR_USER to be able to push to production (mirror) index REGISTRY_RELEASE_API_TOKEN Quay application Token (user: $oauth ) used for pushing to release index REPO_GHA_PAT GitHub access token (deprecated) OHIO_REGISTRY_TOKEN Quay application token (user: $oauth ) to push index image used for operatorhub.io web page SIGNATURE_WEBHOOK_PASSWD Index signrature password SIGNATURE_WEBHOOK_REQUESTER_EMAIL Index signrature requester email SIGNATURE_WEBHOOK_SECRET Index signrature secret","title":"Secrets"},{"location":"project/setup/#generate-github-action-workflows","text":"Note On newly created projects one should copy the file ( upgrade.yaml ) to .github/workflows/upgrade.yaml and push it into main branch so CI Upgrade workflow is enabled in the Actions tab in the GitHub project. After the directory and configurations are in place one can generate all workflows by running Upgrade Action from the configured project. More details one can find here One can verify Upgrade CI GitHub Action. See screenshot below","title":"Generate Github Action workflows"}]}